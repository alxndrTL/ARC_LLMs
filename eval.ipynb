{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "openai.api_key = \"YOUR_OPENAI_KEY\"\n",
    "\n",
    "model = \"gpt-3.5-turbo-instruct\"\n",
    "token_limit = 4096\n",
    "\n",
    "def LLM(prompt, prompt_len, temperature=0):\n",
    "  responses = openai.Completion.create(engine=model, prompt=prompt, max_tokens=token_limit-prompt_len, temperature=temperature, stop=['\\n---'])\n",
    "  text = [response['text'] for response in responses['choices']]\n",
    "  return text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import openai\n",
    "\n",
    "openai.api_key = \"YOUR_OPENAI_KEY\"\n",
    "\n",
    "\n",
    "def LLM(prompt, prompt_len, temperature=0):\n",
    "    model = \"gpt-3.5-turbo\"\n",
    "  \n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=temperature)\n",
    "    text = response['choices'][0]['message']['content']\n",
    "    return text\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet = [str(i) for i in range(10)] # classic\n",
    "#alphabet = ['G', 'D', 'E', 'B', 'A', 'C', 'F', 'H', 'I', 'J']\n",
    "value_to_token = lambda x: {i:a for i, a in enumerate(alphabet)}[x]\n",
    "token_to_value = lambda x: {a:i for i, a in enumerate(alphabet)}[x]\n",
    "\n",
    "def replace_numbers_with_tokens(s):\n",
    "    # Loop through each character in the string\n",
    "    result = []\n",
    "    for char in s:\n",
    "        if char.isdigit():  # Check if character is a digit\n",
    "            token = value_to_token(int(char))\n",
    "            result.append(token)\n",
    "        else:\n",
    "            result.append(char)\n",
    "    return ''.join(result)\n",
    "\n",
    "def replace_tokens_with_numbers(s):\n",
    "    result = []\n",
    "    index = 0\n",
    "    while index < len(s):\n",
    "        if s[index] in alphabet:\n",
    "            number = token_to_value(s[index])\n",
    "            result.append(str(number))\n",
    "        else:\n",
    "            result.append(s[index])\n",
    "        index += 1\n",
    "    return ''.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from the code of the \"LLMs as general pattern machines\" paper\n",
    "colors = [(0, 0, 0),\n",
    "          (0, 116, 217),\n",
    "          (255, 65, 54),\n",
    "          (46, 204, 6),\n",
    "          (255, 220, 0),\n",
    "          (170, 170, 170),\n",
    "          (240, 18, 190),\n",
    "          (255, 133, 27),\n",
    "          (127, 219, 255),\n",
    "          (135, 12, 37)]\n",
    "\n",
    "def grid_to_img(grid):\n",
    "  grid = np.int32(grid)\n",
    "  scale = 10\n",
    "  img = np.zeros((grid.shape[0] * scale + 1, grid.shape[1] * scale + 1, 3), dtype=np.uint8)\n",
    "  for r in range(grid.shape[0]):\n",
    "    for c in range(grid.shape[1]):\n",
    "      img[r*scale+1:(r+1)*scale, c*scale+1:(c+1)*scale, :] = colors[grid[r, c]]\n",
    "  new_img = img.copy()\n",
    "  new_img[0::10, :, :] = np.uint8(np.round((0.7 * np.float32(img[0::10, :, :]) + 0.3 * 255)))\n",
    "  new_img[:, 0::10, :] = np.uint8(np.round((0.7 * np.float32(img[:, 0::10, :]) + 0.3 * 255)))\n",
    "  return new_img\n",
    "\n",
    "def show_task(task):\n",
    "  print(\"TRAIN:\")\n",
    "  for i, ex in enumerate(task[\"train\"]):\n",
    "    in_img = grid_to_img(ex[\"input\"])\n",
    "    out_img = grid_to_img(ex[\"output\"])\n",
    "    plt.subplot(1, 2, 1); plt.imshow(grid_to_img(ex[\"input\"]))\n",
    "    plt.subplot(1, 2, 2); plt.imshow(grid_to_img(ex[\"output\"]))\n",
    "    plt.show()\n",
    "  print(\"TEST:\")\n",
    "  for i, ex in enumerate([task[\"test\"][0]]): #only display the first test task, as its only this task thats sent to the model\n",
    "    in_img = grid_to_img(ex[\"input\"])\n",
    "    out_img = grid_to_img(ex[\"output\"])\n",
    "    plt.subplot(1, 2, 1); plt.imshow(grid_to_img(ex[\"input\"]))\n",
    "    plt.subplot(1, 2, 2); plt.imshow(grid_to_img(ex[\"output\"]))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_task(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        task = json.load(f)\n",
    "    return task\n",
    "\n",
    "def process_arc_task(task):\n",
    "    task_np = {\n",
    "        \"train\": [{\"input\": np.array(item[\"input\"]), \"output\": np.array(item[\"output\"])} for item in task[\"train\"]],\n",
    "        \"test\": [{\"input\": np.array(item[\"input\"]), \"output\": np.array(item[\"output\"])} for item in task[\"test\"]]\n",
    "    }\n",
    "    return task_np\n",
    "\n",
    "def numpy_array_to_string(arr):\n",
    "    return '\\n'.join([','.join(map(str, row)) for row in arr])\n",
    "\n",
    "def string_to_numpy_array(s):\n",
    "    return np.array([list(map(int, row.split(','))) for row in s.split('\\n')])\n",
    "\n",
    "def task_to_prompt(task):\n",
    "    prompt = \"\"\n",
    "\n",
    "    for demo in task['train']:\n",
    "        prompt += \"input:\\n\"\n",
    "        prompt += numpy_array_to_string(demo['input'])\n",
    "        prompt += \"\\n\"\n",
    "        prompt += \"output:\\n\"\n",
    "        prompt += numpy_array_to_string(demo['output'])\n",
    "        prompt += \"\\n---\\n\"\n",
    "\n",
    "    prompt += \"input:\\n\"\n",
    "    prompt += numpy_array_to_string(task['test'][0]['input'])\n",
    "    prompt += \"\\n\"\n",
    "    prompt += \"output:\\n\"\n",
    "    return prompt\n",
    "\n",
    "tasks_training = os.listdir(\"ARC/data/training/\")\n",
    "tasks_evaluation = os.listdir(\"ARC/data/evaluation/\")\n",
    "\n",
    "tasks_json = []\n",
    "\n",
    "for file in [os.path.join(\"ARC/data/training/\", file) for file in tasks_training]:\n",
    "    tasks_json.append(load_task(file))\n",
    "\n",
    "for file in [os.path.join(\"ARC/data/evaluation/\", file) for file in tasks_evaluation]:\n",
    "    tasks_json.append(load_task(file))\n",
    "\n",
    "#tasks_ids = [6, 14, 33, 34, 37, 59, 72, 79, 85, 89, 96]\n",
    "#tasks_json = [tasks_json[i] for i in tasks_ids]\n",
    "\n",
    "tasks_ids = list(range(200, 400))\n",
    "tasks_json = [tasks_json[i] for i in tasks_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 False\n",
      "201 False\n",
      "202 False\n",
      "203 False\n",
      "204 True\n",
      "205 False\n",
      "206 False\n",
      "207 False\n",
      "208 False\n",
      "209 task too long (prompt len=6246)\n",
      "210 False\n",
      "211 task too long (prompt len=5672)\n",
      "212 False\n",
      "213 False\n",
      "214 False\n",
      "215 False\n",
      "216 False\n",
      "217 True\n",
      "218 False\n",
      "219 False\n",
      "220 False\n",
      "221 True\n",
      "222 False\n",
      "223 False\n",
      "224 False\n",
      "225 False\n",
      "226 True\n",
      "227 False\n",
      "228 False\n",
      "229 False\n",
      "230 False\n",
      "231 False\n",
      "232 True\n",
      "233 False\n",
      "234 False\n",
      "235 False\n",
      "236 False\n",
      "237 False\n",
      "238 False\n",
      "239 False\n",
      "240 task too long (prompt len=5672)\n",
      "241 False\n",
      "242 False\n",
      "243 False\n",
      "244 False\n",
      "245 False\n",
      "246 True\n",
      "247 True\n",
      "248 False\n",
      "249 False\n",
      "250 False\n",
      "251 True\n",
      "252 task too long (prompt len=7988)\n",
      "253 False\n",
      "254 decoding error (task len : 3283)\n",
      "255 True\n",
      "256 task too long (prompt len=16291)\n",
      "257 False\n",
      "258 False\n",
      "259 True\n",
      "260 False\n",
      "261 False\n",
      "262 False\n",
      "263 False\n",
      "264 False\n",
      "265 False\n",
      "266 False\n",
      "267 True\n",
      "268 False\n",
      "269 False\n",
      "270 False\n",
      "271 False\n",
      "272 False\n",
      "273 False\n",
      "274 False\n",
      "275 False\n",
      "276 False\n",
      "277 False\n",
      "278 False\n",
      "279 False\n",
      "280 False\n",
      "281 False\n",
      "282 False\n",
      "283 True\n",
      "284 False\n",
      "285 False\n",
      "286 False\n",
      "287 False\n",
      "288 task too long (prompt len=4228)\n",
      "289 False\n",
      "290 task too long (prompt len=12672)\n",
      "291 False\n",
      "292 False\n",
      "293 False\n",
      "294 False\n",
      "295 False\n",
      "296 True\n",
      "297 False\n",
      "298 False\n",
      "299 True\n",
      "300 False\n",
      "301 False\n",
      "302 False\n",
      "303 False\n",
      "304 False\n",
      "305 False\n",
      "306 decoding error (task len : 3854)\n",
      "307 task too long (prompt len=5455)\n",
      "308 False\n",
      "309 False\n",
      "310 False\n",
      "311 False\n",
      "312 True\n",
      "313 False\n",
      "314 False\n",
      "315 False\n",
      "316 task too long (prompt len=4118)\n",
      "317 False\n",
      "318 False\n",
      "319 False\n",
      "320 False\n",
      "321 False\n",
      "322 decoding error (task len : 3626)\n",
      "323 False\n",
      "324 False\n",
      "325 False\n",
      "326 False\n",
      "327 False\n",
      "328 False\n",
      "329 False\n",
      "330 False\n",
      "331 True\n",
      "332 False\n",
      "333 False\n",
      "334 False\n",
      "335 False\n",
      "336 decoding error (task len : 3481)\n",
      "337 False\n",
      "338 False\n",
      "339 False\n",
      "340 False\n",
      "341 False\n",
      "342 False\n",
      "343 False\n",
      "344 False\n",
      "345 False\n",
      "346 True\n",
      "347 False\n",
      "348 False\n",
      "349 False\n",
      "350 False\n",
      "351 False\n",
      "352 False\n",
      "353 False\n",
      "354 False\n",
      "355 False\n",
      "356 False\n",
      "357 False\n",
      "358 False\n",
      "359 False\n",
      "360 task too long (prompt len=9613)\n",
      "361 False\n",
      "362 False\n",
      "363 task too long (prompt len=6246)\n",
      "364 True\n",
      "365 False\n",
      "366 False\n",
      "367 False\n",
      "368 False\n",
      "369 False\n",
      "370 False\n",
      "371 False\n",
      "372 task too long (prompt len=9890)\n",
      "373 False\n",
      "374 False\n",
      "375 False\n",
      "376 False\n",
      "377 False\n",
      "378 False\n",
      "379 False\n",
      "380 False\n",
      "381 False\n",
      "382 False\n",
      "383 False\n",
      "384 False\n",
      "385 False\n",
      "386 False\n",
      "387 False\n",
      "388 False\n",
      "389 False\n",
      "390 False\n",
      "391 False\n",
      "392 True\n",
      "393 True\n",
      "394 False\n",
      "395 False\n",
      "396 False\n",
      "397 False\n",
      "398 False\n",
      "399 False\n"
     ]
    }
   ],
   "source": [
    "responses = {}\n",
    "\n",
    "for id, task in enumerate(tasks_json):\n",
    "    task_np = process_arc_task(task)\n",
    "    prompt = task_to_prompt(task_np)\n",
    "    prompt = replace_numbers_with_tokens(prompt)\n",
    "\n",
    "    if len(prompt) > 4000:\n",
    "        print(str(id+200) + \" task too long (prompt len={})\".format(len(prompt)))\n",
    "        continue\n",
    "\n",
    "    response = LLM(prompt, len(prompt))\n",
    "    response = replace_tokens_with_numbers(response)\n",
    "    responses[id] = response\n",
    "    \n",
    "    try:\n",
    "        response_np = string_to_numpy_array(response)\n",
    "    except Exception as e:\n",
    "        print(str(id+200) + \" decoding error (task len : {})\".format(len(prompt)))\n",
    "        continue\n",
    "\n",
    "    correct_np = np.array(task_np['test'][0]['output'])\n",
    "    print(str(id+200) + \" \" + str(np.array_equal(response_np, correct_np)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch23",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
